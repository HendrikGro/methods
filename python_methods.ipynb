{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1debc16-0086-44ea-9e2c-69dbc55825d6",
   "metadata": {},
   "source": [
    "# find index of closest values in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d5e8a1-0c3c-4949-941a-e573dd6ca524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = np.unravel_index(np.abs(array - value).argmin(), array.shape)\n",
    "    return array[idx],idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cebac-1ff4-4d4e-a2f4-5d84dc82cfdf",
   "metadata": {},
   "source": [
    "# shift lon from 0-360 to -180,180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466654a4-e959-48b6-aeb4-7703482983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    ".assign_coords({\"lon\": (((data.lon + 180) % 360) - 180)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb232d32-d00f-449b-b58f-eb68f943798c",
   "metadata": {},
   "source": [
    "# datetime format from datestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeac92d-0e80-4d31-9456-ee11dd1952f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime('{}'.format(data.time[i].values), '%Y/%m/%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd34cc-7ba0-4ef0-a52f-a7d85cb6d168",
   "metadata": {},
   "source": [
    "# update font size for whole plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eee488-fac3-4e4e-94d8-0bf03a8a7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':14})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c408e09-5b06-4777-b5aa-ad20b0dcf4e6",
   "metadata": {},
   "source": [
    "# get list of all values in array of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618afcd2-34fb-4a6a-a922-3fc7f48e0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890da5f-e516-4dbf-a4d6-d6fc7c26b066",
   "metadata": {},
   "source": [
    "# value in two different arrays? -> boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542af859-2cf8-4614-8495-d2a87e06f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.in1d(data1,data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b71f52-1299-4d21-bdad-91d143a8d8fc",
   "metadata": {},
   "source": [
    "# cartopy projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcac6e-b277-48ab-aac9-67c63a14d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min = 45\n",
    "lat_max = 65\n",
    "lon_min = -60\n",
    "lon_max = -40\n",
    "resolution = '10m'\n",
    "central_lon, central_lat = (lon_max+lon_min)/2, (lat_max-lat_min)/2\n",
    "extent = [lon_min, lon_max, lat_min, lat_max]\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10),subplot_kw=dict(projection=ccrs.Orthographic(central_lon, central_lat)))\n",
    "ax.coastlines()\n",
    "ax.set_extent(extent)\n",
    "gl = ax.gridlines(draw_labels=True,color='grey')\n",
    "gl.xlocator= mticker.FixedLocator(np.arange(-60,-35,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedc444-8980-43e3-b8d5-cfc2889d1351",
   "metadata": {},
   "source": [
    "# calculate anomalies in xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8644f9-8330-4846-a16f-687236630af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ano = data.groupby('time.dayofyear') - data_mean.rename({'doy':'dayofyear'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6de3e-01b7-4b24-9a0d-7740527dd137",
   "metadata": {},
   "source": [
    "# custom legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aba9b1-0138-4c15-ad1f-eaa94b29ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "colors = ['tab:red', 'tab:blue']\n",
    "lines = [Line2D([0], [0], color=c, linewidth=3) for c in colors]\n",
    "labels = [ 'temperature', 'salinity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf95fd1-567f-4171-bd85-de6a97f63bb4",
   "metadata": {},
   "source": [
    "# add colorbar axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4971eec-18b8-46f0-98c3-18c3fb21dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to plot\n",
    "cax = fig.add_axes([0.25,0.1,0.5,0.033]) \n",
    "cbar = fig.colorbar(plot,orientation='horizontal',cax=cax)\n",
    "cbar.set_label(r'$\\sigma_T$ in $^\\circ$C',fontsize=14)\n",
    "cbar.set_ticks(np.arange(vmin,vmax+1,1))\n",
    "\n",
    "# completely custom\n",
    "m = plt.cm.ScalarMappable(cmap=cmocean.cm.balance)\n",
    "m.set_clim(-0.25,0.25)\n",
    "cax = fig.add_axes([0.25,-0.04,0.5,0.033]) \n",
    "cbar = fig.colorbar(m,orientation='horizontal',cax=cax,boundaries=np.arange(-0.25,0.3,0.05))\n",
    "cbar.set_label(r'velocity [m/s]')\n",
    "cbar.set_ticks(np.arange(-0.25,0.3,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db610fd7-659c-4581-afff-c2a2523d54d8",
   "metadata": {},
   "source": [
    "# distance between two lat/lon points along section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef618c8-f652-41aa-a434-01870e87a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = [0]\n",
    "R = 6371e3\n",
    "lat = ole_orca.lat*np.pi/180\n",
    "lon = ole_orca.lon*np.pi/180\n",
    "for i in np.arange(1,78):\n",
    "    dlat = lat[i] - lat[i-1]\n",
    "    dlon = lon[i] - lon[i-1]\n",
    "    a = np.sin(dlat/2)*np.sin(dlat/2) + np.cos(lat[i])*np.cos(lat[i-1]) * np.sin(dlon/2)*np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    distance.append((R*c).values)\n",
    "model_distance = np.cumsum(distance)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23839bab-f351-426e-ae36-5cc2962493a8",
   "metadata": {},
   "source": [
    "# interpolate on other grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b7881-c714-4d28-a63d-d636ee37c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_viking = np.full_like(viking[0,:,:], np.nan)\n",
    "bathy_viking[:,:] = griddata((bathy.nav_lon.values.ravel(), bathy.nav_lat.values.ravel()),\n",
    "                            bathy[:,:].values.ravel(),\n",
    "                            (viking.nav_lon.values, viking.nav_lat.values),\n",
    "                            fill_value=np.nan,\n",
    "                            method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd82b43-424b-4490-b6e2-13f47518ad3a",
   "metadata": {},
   "source": [
    "# detrend xarray non-uniform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a82920-ccce-4514-afce-8677a902954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean_xarray(da,dim):\n",
    "    return da - da.mean(dim=dim),da.mean(dim=dim)\n",
    "\n",
    "def detrend_xarray(da):\n",
    "    '''\n",
    "    apply detrend along time axis of 2D Xarray\n",
    "    see polynomial_detrend for details about detrending\n",
    "    '''\n",
    "    dt = xr.apply_ufunc(\n",
    "                polynomial_detrend,\n",
    "                da,\n",
    "                input_core_dims=[['time']],\n",
    "                output_core_dims=[['time','dim0']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=[da.dtype],\n",
    "                dask=\"parallelized\",\n",
    "            )\n",
    "    if 'x' in da.dims:\n",
    "        dt = dt.transpose('time','y','x','dim0')\n",
    "    elif 'lon' in da.dims:\n",
    "        dt = dt.transpose('time','lat','lon','dim0')\n",
    "    \n",
    "    return dt.isel(dim0=0),dt.isel(dim0=1)\n",
    "\n",
    "def polynomial_detrend(da,order=1):\n",
    "    '''\n",
    "    detrend of non uniform data on a uniform grid with nan values\n",
    "    \n",
    "    change order to increase order of fittet polynomial, default is 1, so linear\n",
    "    \n",
    "    returns detrended data and trend values\n",
    "    '''\n",
    "    ds = da.copy()\n",
    "    mask = ~np.isnan(ds)\n",
    "    if mask.sum() == 0:\n",
    "        return np.stack((ds,ds),axis=-1)\n",
    "    else:\n",
    "        ds_masked = ds[mask]\n",
    "        time = np.arange(0,len(ds))\n",
    "        time_masked = time[mask]\n",
    "        coeff = np.polyfit(time_masked, ds_masked, order)\n",
    "        trend_nonan = np.polyval(coeff, time_masked)\n",
    "        detrended = ds_masked - trend_nonan\n",
    "        ds[mask] = detrended\n",
    "        trend = np.copy(ds)\n",
    "        trend[mask] = trend_nonan\n",
    "        \n",
    "        return np.stack((ds.data,trend),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d802bfc-0812-4a4d-b27c-a238b9fe2212",
   "metadata": {},
   "source": [
    "# non-uniform discrete fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf115f2-687b-47fa-bf33-bc80d5d0bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nufft(data,xarray_apply=True):\n",
    "    '''\n",
    "    does a non-uniform fast fourier transform on data with a uniform grid but nan values in it\n",
    "    \n",
    "    returns freq: frequency in cycles per timestep (cph for hourly data)\n",
    "            f_k: amplitude for each wavenumber k as a complex number\n",
    "            ps: power spectrum\n",
    "            psd: power spectral density\n",
    "            \n",
    "    xarray_apply is used for the spectral_analysis function for apply_ufunc along time dimension for each point of spatial array\n",
    "        if you just want to do it for one array time series, set it to False\n",
    "    \n",
    "    function is taken from https://github.com/jakevdp/nfft\n",
    "    '''\n",
    "    mask = ~np.isnan(data)\n",
    "    \n",
    "    N_freq = len(data)\n",
    "    k = -N_freq//2 + np.arange(N_freq)\n",
    "    \n",
    "    data_masked = (data)[mask]\n",
    "    t = np.linspace(0, 1,N_freq)[mask]\n",
    "    \n",
    "    f_k = nfft.nfft_adjoint(t,data_masked,N_freq)\n",
    "    ps = np.abs(f_k)**2/N_freq**2\n",
    "    psd = ps * N_freq\n",
    "    freq = k/N_freq\n",
    "    if xarray_apply == True:\n",
    "        t_return = np.linspace(0, 1,N_freq)\n",
    "        t_return[~mask] = np.nan\n",
    "        return np.stack((freq,f_k,ps,psd,t_return),axis=-1)\n",
    "    elif xarray_apply==False:\n",
    "        return freq,f_k,ps,psd,t\n",
    "\n",
    "def spectral_analysis(da):\n",
    "    '''\n",
    "    apply the nufft on each spatial point of a 2D dataset along the timeseries\n",
    "    returns xarray DataSet with frequency spectrum, powerspectrum, power spectral density for each point\n",
    "    t is masked time array without nans, necessary for reconstruction of time series from frequency spectrum\n",
    "    '''\n",
    "    dt = xr.apply_ufunc(\n",
    "                nufft,\n",
    "                da,\n",
    "                input_core_dims=[['time']],\n",
    "                output_core_dims=[['freq','dim0']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=['complex'],\n",
    "                dask=\"parallelized\",\n",
    "                dask_gufunc_kwargs={'output_sizes':{'freq':744,'dim0':5}}\n",
    "            )\n",
    "    dt = xr.Dataset(coords={\n",
    "        'lat':(['y','x'],dt['lat'].data),\n",
    "        'lon':(['y','x'],dt['lon'].data),\n",
    "        'freq':np.real(dt[0,0,:,0].data)\n",
    "    },data_vars={\n",
    "        'f_k':(['y','x','freq'],dt.isel(dim0=1).data),\n",
    "        'ps':(['y','x','freq'],np.real(dt.isel(dim0=2).data)),\n",
    "        'psd':(['y','x','freq'],np.real(dt.isel(dim0=3).data)),\n",
    "        't':(['y','x','freq'],np.real(dt.isel(dim0=4).data))\n",
    "        \n",
    "\n",
    "    })\n",
    "    dt = dt.transpose('freq','y','x')\n",
    "    dt = dt.where(dt!=0)\n",
    "    return dt\n",
    "\n",
    "# spectral_u = spectral_analysis(velocity.u_detrended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2642ed-96ec-44da-86fa-88f0501d8f98",
   "metadata": {},
   "source": [
    "# xmovie example custom plotfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95438a15-fb04-4a32-96fe-8fd18b584cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "def custom_plotfunc(ds,fig,tt,framedim='time',**kwargs):\n",
    "    ax = fig.subplots(subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "    ds.isel({framedim:tt}).plot.quiver('lon','lat','u','v',ax=ax)\n",
    "    ax.add_feature(cfeature.LAND, facecolor='grey',edgecolor='black')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    return None,None\n",
    "    \n",
    "# mov = Movie(velocity,custom_plotfunc,input_check=False)\n",
    "# mov.save('movie_2015.mp4',overwrite_existing=True,framerate=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc161f-e626-4397-93bb-690a213772da",
   "metadata": {},
   "source": [
    "# spatial correlation xarray apply_ufunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebbde2-0ee2-431c-84b7-dad4922daf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import dask\n",
    "\n",
    "#stats.personr only works on dataarrays, so select your variable at one point\n",
    "data = xr.open_dataset('subset.nc').sst \n",
    "\n",
    "#maybe mfdataset helps for huge datasets and then parallization; 39 is the size of time, you want all tim points to be in one chunk\n",
    "# data = xr.open_mfdataset('subset.nc',chunks={'time':39,'lat':100,'lon':100}).sst \n",
    "\n",
    "#stats.pearsonr doesnt allow nan inputs, so put nan to 0; it then returns nan again for correlation along constant arrays\n",
    "data = data.fillna(0)\n",
    "\n",
    "def correlation(data1,data2):\n",
    "    # data1 is the spatial field you want to correlate to\n",
    "    # data2 is your single time series\n",
    "    # calculates the correlation coefficient and p_value\n",
    "    # returns the result as a numpy array, because the initial output of the function is of a weird PearsonRResult class, which doesnt work in apply_ufunc\n",
    "    result = stats.pearsonr(data1,data2)\n",
    "    return np.stack((result[0],result[1]), axis=-1)\n",
    "\n",
    "# apply_ufunc takes the function you want to apply and then the necessary input arguments to that function\n",
    "# so data is your spatial field and then your single time series (I just selected one pointfrom my field)\n",
    "# the input_core_dimensions basically mean along which dimension your function is applied on\n",
    "# the output dimension is necesarry because the correlation output is of size 2\n",
    "# dask='parallelized' makes it faster, but needs some additional arguments for your output\n",
    "\n",
    "result = xr.apply_ufunc(correlation,data,data.isel(lat=50,lon=50),input_core_dims=[['time'],['time']],output_core_dims=[['statistic']],vectorize=True,\n",
    "                        dask='parallelized',output_dtypes=[np.dtype(float)],dask_gufunc_kwargs={'output_sizes':{'statistic':2}})\n",
    "\n",
    "# make xarray dataset of the output, because the output has r and p along one extra dimension, so assign them to single variables\n",
    "statistics = xr.Dataset(coords={'lat':result.lat,'lon':result.lon}, data_vars = {\n",
    "    'corrcoef':result[:,:,0],\n",
    "    'p_value':result[:,:,1]\n",
    "})\n",
    "\n",
    "# necessary if you use mfdatasets, so you finally compute the correlation for each chunk\n",
    "# statistics = statistics.compute()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a0978-77ec-461f-94c9-443fbde1f444",
   "metadata": {},
   "source": [
    "# find peaks in spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f014507-0cfb-4595-9137-52415e80d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finder(freq_data,ps_data,freq_min,freq_max,peak_height):\n",
    "    '''\n",
    "    find amplitude and frequency of maximum peak in frequency range of spectrum\n",
    "    freq_data, ps_data: frequency values and powerspectrum\n",
    "    freq_min, freq_max: bounds of frequency range\n",
    "    peak_height: threshold for peak to detect\n",
    "    \n",
    "    if you want to detect on frequency amplitudes or power spectral density, adjust peak_height accordingly, should work\n",
    "    \n",
    "    returns maximum peak and according frequency\n",
    "    '''\n",
    "    \n",
    "    freq_mask = np.array([freq_data > freq_min]) & np.array([freq_data < freq_max])\n",
    "    freq_mask = np.squeeze(freq_mask)\n",
    "    peaks,peak_heights = scipy.signal.find_peaks(ps_data[freq_mask],height=peak_height)\n",
    "    freq_peaks = freq_data[freq_mask][peaks]\n",
    "    if len(peak_heights['peak_heights']) != 0:\n",
    "        peak_max = max(peak_heights['peak_heights'])\n",
    "        freq_max = freq_peaks[peak_heights['peak_heights'] == peak_max]\n",
    "    else:\n",
    "        peak_max,freq_max = np.nan,np.nan\n",
    "    \n",
    "    return np.array([freq_max,peak_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ba25d-abcf-4aea-8021-e7f29622af9f",
   "metadata": {},
   "source": [
    "# spectral ellipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "faa91c37-8c90-4e51-b6c9-e4b40a858548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse_details(fm,fp):\n",
    "    '''\n",
    "    calculates major and minor axis as well as azimuth angle of spectral ellipse from positive and negative frequency amplitudes\n",
    "    see Walden2012 for mathematical insights\n",
    "    '''\n",
    "    Rp = np.abs(fp) + np.abs(fm)\n",
    "    Rm = np.abs(np.abs(fp) - np.abs(fm))\n",
    "\n",
    "    major = np.abs(Rp + Rm)\n",
    "    minor = np.abs(Rp - Rm)\n",
    "\n",
    "    azimuth = cmath.phase(fp*fm)/2\n",
    "    return major,minor,azimuth\n",
    "\n",
    "major,minor,azimuth = ellipse_details(fm,fp)\n",
    "lonlat = [fm.lon.data ,  fm.lat.data]\n",
    "rot = int(np.abs(fp).data > np.abs(fm).data)\n",
    "ell = mpl.patches.Ellipse(xy=lonlat, width=major/1e4, height=minor/1e4, angle = azimuth*180/np.pi,fc='none',ec=['b','r'][rot],transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_patch(ell)\n",
    "ax.set_aspect('equal')\n",
    "ax.autoscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3575fb-ee34-4da8-a873-dbce454ddd9d",
   "metadata": {},
   "source": [
    "# spectral time filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961a829f-6685-4da5-a468-be61e140cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hann_window(n,T):\n",
    "    '''\n",
    "    calculates the frequency spectrum of a hanning window for n samples leading to n frequencies and T window size\n",
    "    n has to be integer and n % 2 has to be 0\n",
    "    '''\n",
    "    assert n%2==0\n",
    "    w = np.hanning(T)\n",
    "    w = w / w.sum()\n",
    "    freq_hann = np.fft.fftfreq(n)\n",
    "\n",
    "    w_transform = np.abs(np.fft.fft(w, n=n))\n",
    "    mid_index = int(n/2)\n",
    "    amp_hann = np.concatenate((w_transform[mid_index:],w_transform[:mid_index]))\n",
    "    freq_hann = np.concatenate((freq_hann[mid_index:],freq_hann[:mid_index]))\n",
    "    return freq_hann,amp_hann\n",
    "\n",
    "def butterworth_window(n,fc,order):\n",
    "    '''\n",
    "    calculates the frequency spectrum of a butterworth window for n samples leading to n frequencies and fc cut off frequency with order of butterworth filter \n",
    "    n has to be integer and n % 2 has to be 0\n",
    "    fs is sample frequency in Hz, we have hourly data, so 1/3600\n",
    "    '''\n",
    "    \n",
    "    assert n%2==0\n",
    "    fs = 1/3600\n",
    "    nyq = 0.5 * fs\n",
    "    fc = fc * (1/3600)\n",
    "    freqs = (-n//2 + np.arange(n))/n/3600\n",
    "\n",
    "    b,a = scipy.signal.butter(N=order,Wn=fc,btype='lowpass',fs = fs)\n",
    "    freq_but, amp_but = scipy.signal.freqz(b,a,fs=fs,worN=freqs)\n",
    "    return freq_but*3600, amp_but\n",
    "\n",
    "def inverse_nufft(t,f_k):\n",
    "    '''\n",
    "    inverse fourier transform to get back time series from fourier components\n",
    "    non-uniform discrete fourier transform\n",
    "    \n",
    "    returns array of reconstructed velocities on the complete time grid, so with nan values\n",
    "    '''\n",
    "    reconstructed = nfft.nfft(drop_na(t), f_k)/len(f_k)\n",
    "    reconstructed_ontime = np.copy(t)\n",
    "    reconstructed_ontime[~np.isnan(t)] = reconstructed\n",
    "    return reconstructed_ontime\n",
    "\n",
    "def inverse_nufft_window(t,f_k,window_amplitudes):\n",
    "    '''\n",
    "    inverse fourier transform to get back time series from fourier components\n",
    "    non-uniform discrete fourier transform but with amplitude taken out of high frequencies using a hanning window, which has to be predefined\n",
    "    \n",
    "    returns array of reconstructed velocities on the complete time grid, so with nan values\n",
    "    '''\n",
    "    reconstructed = nfft.nfft(drop_na(t), f_k*window_amplitudes)/len(f_k)\n",
    "    reconstructed_ontime = np.copy(t)\n",
    "    reconstructed_ontime[~np.isnan(t)] = reconstructed\n",
    "    return reconstructed_ontime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caaa575-f5eb-47a0-8736-1531d76c6d5c",
   "metadata": {},
   "source": [
    "# unique values in array but keeps order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97670e-b7a5-4959-823d-ac25fa257554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(sequence):\n",
    "    '''\n",
    "    finds unique values of array while keeping the order of values in array\n",
    "    (set() orders values in increasing way)\n",
    "    '''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae6b91-3ed0-4c1c-9e03-d3c179dc3d72",
   "metadata": {},
   "source": [
    "# section indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189b228-0385-441a-88c1-ac8ac2ee5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_indices(data,point1,point2):\n",
    "    '''\n",
    "    get x,y indices of section between latlon point1 and point2\n",
    "    \n",
    "    uses haversine distance to look for closest points on latlon grid to given latlon point\n",
    "    \n",
    "    first defines straight latlon line between point1 and point2\n",
    "    \n",
    "    then looks for closest points on grids to this line\n",
    "    \n",
    "    straight line has higher resolution than grid, so we discard all the x,y points that appear twice\n",
    "    '''\n",
    "    lat = np.asarray(data.lat.data)\n",
    "    lon = np.asarray(data.lon.data)\n",
    "    \n",
    "    lat1,lon1 = point1\n",
    "    lat2,lon2 = point2\n",
    "    \n",
    "\n",
    "    f = np.frompyfunc(haversine,4,1)\n",
    "    value, indices1 = find_nearest(f(lon,lat,lon1,lat1).astype(float),0)\n",
    "    value, indices2 = find_nearest(f(lon,lat,lon2,lat2).astype(float),0)\n",
    "\n",
    "    x_dist = np.abs(indices1[1] - indices2[1]) \n",
    "    y_dist = np.abs(indices1[0] - indices2[0]) \n",
    "\n",
    "    dist_max = max(x_dist,y_dist)\n",
    "\n",
    "    if (lat1-lat2) == 0:\n",
    "        longitudes = np.linspace(lon1, lon2, dist_max*2)\n",
    "        latitudes = np.ones(dist_max*2)*lat1\n",
    "    elif (lon1-lon2) == 0:\n",
    "        latitudes = np.linspace(lat1, lat2, dist_max*2)\n",
    "        longitudes = np.ones(dist_max*2)*lon1\n",
    "    else:\n",
    "        latitudes = np.linspace(lat1, lat2, dist_max*2)\n",
    "        longitudes = (lon2 - lon1)/(lat2 - lat1)*(latitudes - lat1) + lon1\n",
    "\n",
    "    # x_idx = []\n",
    "    # y_idx = []\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(dist_max*2):\n",
    "        dummy = find_nearest(f(lon,lat,longitudes[i],latitudes[i]).astype(float),0)[1]\n",
    "        # x_idx.append(dummy[1])\n",
    "        # y_idx.append(dummy[0])\n",
    "        indices.append((dummy[1],dummy[0]))\n",
    "    indices = unique(indices)\n",
    "    x_idx = [x[0] for x in indices]\n",
    "    y_idx = [x[1] for x in indices]\n",
    "    \n",
    "    return x_idx,y_idx,latitudes,longitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db53533-9631-4f65-8854-2809bf77a92a",
   "metadata": {},
   "source": [
    "# rotate velocities on section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9099a8cf-28aa-4aa1-8912-82017df133b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_initial_compass_bearing(pointA, pointB):\n",
    "    \"\"\"\n",
    "    Calculates the bearing between two points.\n",
    "    The formulae used is the following:\n",
    "        θ = atan2(sin(Δlong).cos(lat2),\n",
    "                  cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))\n",
    "    :Parameters:\n",
    "      - `pointA: The tuple representing the latitude/longitude for the\n",
    "        first point. Latitude and longitude must be in decimal degrees\n",
    "      - `pointB: The tuple representing the latitude/longitude for the\n",
    "        second point. Latitude and longitude must be in decimal degrees\n",
    "    :Returns:\n",
    "      The bearing in degrees\n",
    "    :Returns Type:\n",
    "      float\n",
    "    \"\"\"\n",
    "    if (type(pointA) != tuple) or (type(pointB) != tuple):\n",
    "        raise TypeError(\"Only tuples are supported as arguments\")\n",
    "\n",
    "    lat1 = math.radians(pointA[0])\n",
    "    lat2 = math.radians(pointB[0])\n",
    "\n",
    "    diffLong = math.radians(pointB[1] - pointA[1])\n",
    "\n",
    "    x = math.sin(diffLong) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)\n",
    "            * math.cos(lat2) * math.cos(diffLong))\n",
    "\n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # Now we have the initial bearing but math.atan2 return values\n",
    "    # from -180° to + 180° which is not what we want for a compass bearing\n",
    "    # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "    return compass_bearing\n",
    "\n",
    "def rotate_velocities(u,v,angle):\n",
    "    '''\n",
    "    rotates velocity vector in mathematical positive direction\n",
    "    '''\n",
    "    \n",
    "    u_ = u * np.cos(angle) - v * np.sin(angle)\n",
    "    v_ = u * np.sin(angle) + v * np.cos(angle)\n",
    "    \n",
    "    return u_,v_\n",
    "\n",
    "def rotate_section(section):\n",
    "    '''\n",
    "    rotate velocities on section to cross and along section velocities\n",
    "    \n",
    "    !!! This heavily depends on how you defined the section !!!\n",
    "    \n",
    "    The initial rotation angle is calculated as the compass bearing between two latlon points. I then substract 90deg and change the sign.\n",
    "    This leads to a clockwise rotation with the angle being between the cartesian axis and the section. However, the\n",
    "    initinal angle depends on which one is entered first, leading to a difference of 180deg.\n",
    "    So check the compass bearing between your point first and think about if this leads to the velocites ending up in the way you want.\n",
    "    I wanted u to be mostly eastward and v to be mostly northward\n",
    "    \n",
    "    '''\n",
    "    point1 = (section.isel(s=0).lat.data,section.isel(s=0).lon.data)\n",
    "    point2 = (section.isel(s=-1).lat.data,section.isel(s=-1).lon.data)\n",
    "    \n",
    "    angle = np.deg2rad(calculate_initial_compass_bearing(point1,point2))\n",
    "    angle = angle = -(angle- np.pi/2)\n",
    "    \n",
    "    section['cross_vel'],section['along_vel'] = rotate_velocities(section.u_filtered,section.v_filtered,angle)\n",
    "    return section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf078af-ad64-45fb-bfe9-9bbd9cd6df6b",
   "metadata": {},
   "source": [
    "# velocity vector time series correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "63dc5a47-99d8-4b4d-9390-3988d6799df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_correlation(u1, v1, u2, v2, lag=0, time=None, dt=''):\n",
    "    \"\"\"\n",
    "    calculates the correlation and average veering angle between two velocity \n",
    "    vector time series\n",
    "    following the method from Kundu 1976:\n",
    "    https://journals.ametsoc.org/view/journals/phoc/6/2/1520-0485_1976_006_0238_evonto_2_0_co_2.xml\n",
    "    veering angle is counterclockwise angle of the second vector with respect \n",
    "    to the first\n",
    "\n",
    "    lagged correlation shifts first time series in positive time direction\n",
    "\n",
    "    input:\n",
    "        velocity component time series as numpy array\n",
    "        if you want lagged correlation:\n",
    "            lag: size of lag\n",
    "            time: time array as np.datetime64[ns] array\n",
    "            dt: sampling frequency, has to be uniform\n",
    "    returns:\n",
    "        correlation and average veering angle in degrees\n",
    "    example:\n",
    "        pure eastward and pure northward flow: r=1, angle=90deg\n",
    "    \"\"\"\n",
    "\n",
    "    mask = ~np.isnan(u1)\n",
    "    u1 = u1[mask][lag:]\n",
    "    v1 = v1[mask][lag:]\n",
    "\n",
    "    if lag == 0:\n",
    "        u2 = u2[mask]\n",
    "        v2 = v2[mask]\n",
    "    elif lag != 0:\n",
    "        # assert time, 'time array needed to do lags'\n",
    "        u2 = u2[np.in1d(time, time[mask][lag:] - np.timedelta64(lag, dt))]\n",
    "        v2 = v2[np.in1d(time, time[mask][lag:] - np.timedelta64(lag, dt))]\n",
    "\n",
    "    w1 = u1 + 1j*v1\n",
    "    w1 = w1\n",
    "    w2 = u2 + 1j*v2\n",
    "    w2 = w2\n",
    "\n",
    "    rho = np.mean(np.conjugate(w1)*w2)/np.sqrt(np.mean(np.conjugate(w1)*w1))/\\\n",
    "        np.sqrt(np.mean(np.conjugate(w2)*w2))\n",
    "\n",
    "    return np.array((abs(rho), np.degrees(np.angle(rho))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ad89b-ab24-4153-b123-68b718b22e55",
   "metadata": {},
   "source": [
    "# Cross Spectrum for non-uniform time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d86fbd-3520-4a14-8d6f-2ea9eb36619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossSpectrum(x, y, nperseg):\n",
    "    '''\n",
    "    calculate the cross spectrum between two time series\n",
    "    uses a non-uniform FFT for spectral amplitudes\n",
    "    applies welch's method with a hanning window of size nperseg\n",
    "    and 50% overlap\n",
    "    \n",
    "    input:\n",
    "        x,y: time series data as numpy array\n",
    "        nperseg: window size for welch's method\n",
    "    returns:\n",
    "        cross-spectral amplitudes\n",
    "        frequencies\n",
    "    '''\n",
    "    cross = np.zeros(nperseg, dtype='complex128')\n",
    "    for ind in np.arange(0, int(len(x)/nperseg)-0.5, 0.5):\n",
    "        xp = x[int(ind * nperseg): int((ind + 1)*nperseg)]\n",
    "        xp = xp - np.mean(xp)\n",
    "        xp = xp*np.hanning(nperseg)\n",
    "\n",
    "        yp = y[int(ind * nperseg): int((ind + 1)*nperseg)]\n",
    "        yp = yp - np.mean(yp)\n",
    "        yp = yp*np.hanning(nperseg)\n",
    "\n",
    "        # Do FFT\n",
    "        freqxi, cfxi, psxi, psdxi, txi = nufft(xp, xarray_apply=False)\n",
    "        freqyi, cfyi, psyi, psdyi, tyi = nufft(yp, xarray_apply=False)\n",
    "\n",
    "    # Get cross spectrum\n",
    "        cross += cfxi.conj()*cfyi\n",
    "    freq = (-nperseg//2 + np.arange(nperseg))/nperseg\n",
    "    return np.stack((cross, freq), axis=-1)\n",
    "\n",
    "# coherence = abs(Pxy)**2/(Pxx*Pyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a8757-48b8-42af-822c-c3dbc8a89f52",
   "metadata": {},
   "source": [
    "# convert txt of jupyter notebook to .ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403b09fc-0b5a-4d9d-b9a3-733088badb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save txt as .py file\n",
    "\n",
    "import nbformat.current as convert\n",
    "\n",
    "conv = convert.read(open('untitled.py', 'r'), 'py')\n",
    "convert.write(conv, open('Ruehs_Winds.ipynb', 'w'), 'ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0ad90-1b2b-4162-bf54-aa896e1b03c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
